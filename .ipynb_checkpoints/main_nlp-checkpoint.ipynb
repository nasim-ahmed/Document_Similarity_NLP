{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (3.5)\n",
      "Requirement already satisfied: regex in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from nltk) (2020.10.11)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from nltk) (0.14.1)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from nltk) (7.1.1)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from nltk) (4.50.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.2; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: gensim in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (3.8.3)\n",
      "Requirement already satisfied: six>=1.5.0 in /Users/nasim/Library/Python/3.8/lib/python/site-packages (from gensim) (1.14.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from gensim) (3.0.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from gensim) (1.18.2)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from smart-open>=1.8.1->gensim) (2.23.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim) (2.9)\n",
      "\u001b[33mWARNING: You are using pip version 20.2; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/nasim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'sun', 'rises', 'in', 'the', 'Ease', 'and', 'sets', 'in', 'the', 'west', '.']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Word tokenizations which basically splits a sentence into words\"\"\"\n",
    "\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "data = \"The sun rises in the East and sets in the west.\"\n",
    "print(word_tokenize(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Earth is not a perfect sphere.', \"As Earth spins, gravity points toward the center of our planet (assuming for explanation's sake that Earth is a perfect sphere), and a centrifugal force pushes outward.\"]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Sentence tokenization is needed to count the average words per sentence. So we need to use to calculate the ratio\"\"\"\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "data = \"Earth is not a perfect sphere. As Earth spins, gravity points toward the center of our planet (assuming for explanation's sake that Earth is a perfect sphere), and a centrifugal force pushes outward.\"\n",
    "print(sent_tokenize(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 4\n",
      "['Vehicles with no human driver on board were previously tested by a select group of Waymo customers, before the pandemic halted the service.', 'The driver-free cars will initially be available only to friends and family of those early Waymo adopters, but will extend to all app users within weeks.', 'The service comes two years later than originally promised.', 'The cars will be remotely monitored, with vehicle operators intervening in extreme circumstances - such as a road ahead being closed.']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"File opening and sentence tokenization\"\"\"\n",
    "\n",
    "import nltk, gensim\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "\n",
    "\n",
    "file_docs = []\n",
    "\n",
    "with open('document.txt') as f:\n",
    "    tokens = sent_tokenize(f.read())\n",
    "    for line in tokens: \n",
    "        file_docs.append(line)\n",
    "        \n",
    "print(\"Number of documents:\", len(file_docs))\n",
    "print(file_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vehicles', 'with', 'no', 'human', 'driver', 'on', 'board', 'were', 'previously', 'tested', 'by', 'a', 'select', 'group', 'of', 'waymo', 'customers', ',', 'before', 'the', 'pandemic', 'halted', 'the', 'service', '.']\n",
      "{',': 0, '.': 1, 'a': 2, 'before': 3, 'board': 4, 'by': 5, 'customers': 6, 'driver': 7, 'group': 8, 'halted': 9, 'human': 10, 'no': 11, 'of': 12, 'on': 13, 'pandemic': 14, 'previously': 15, 'select': 16, 'service': 17, 'tested': 18, 'the': 19, 'vehicles': 20, 'waymo': 21, 'were': 22, 'with': 23, 'adopters': 24, 'all': 25, 'and': 26, 'app': 27, 'available': 28, 'be': 29, 'but': 30, 'cars': 31, 'driver-free': 32, 'early': 33, 'extend': 34, 'family': 35, 'friends': 36, 'initially': 37, 'only': 38, 'those': 39, 'to': 40, 'users': 41, 'weeks': 42, 'will': 43, 'within': 44, 'comes': 45, 'later': 46, 'originally': 47, 'promised': 48, 'than': 49, 'two': 50, 'years': 51, '-': 52, 'ahead': 53, 'as': 54, 'being': 55, 'circumstances': 56, 'closed': 57, 'extreme': 58, 'in': 59, 'intervening': 60, 'monitored': 61, 'operators': 62, 'remotely': 63, 'road': 64, 'such': 65, 'vehicle': 66}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"The next step is to tokenize into words and create a dictionary. It is important to convert the tokens into unique ids \n",
    "which then allows Genism to create a Dictionary object that maps each word to unique id\"\"\"\n",
    "\n",
    "gen_docs = [[w.lower() for w in word_tokenize(text)] for text in file_docs]\n",
    "print(gen_docs[0])\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 2), (20, 1), (21, 1), (22, 1), (23, 1)]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Bag of words. (Lists the number of times each word occurs in the sentence)\"\"\"\n",
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-62079b633db7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtf_idf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTfidfModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf_idf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-62079b633db7>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtf_idf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTfidfModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf_idf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"TFIDF(Term Frequency and Inverse Document Frequency)\n",
    "It is a measure of originality of a word by comparing  the no of times a word appears in a doc with the number of docs the word appears in.\n",
    "Basically words that occur more frequently across the documents get smaller weights\"\"\"\n",
    "\n",
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "for doc in tf_idf[corpus]:\n",
    "    print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
